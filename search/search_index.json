{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"torch_jax_interop","text":""},{"location":"#installation","title":"Installation","text":"<ol> <li> <p>(optional) Install UV: https://docs.astral.sh/uv/getting-started/installation/</p> </li> <li> <p>Install this package:</p> </li> </ol> <pre><code>uv add torch_jax_interop\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>Tools to help interoperability between PyTorch and Jax code.</p>"},{"location":"#torch_jax_interop--examples","title":"Examples","text":""},{"location":"#torch_jax_interop--converting-torchtensors-into-jaxarrays-and-vice-versa","title":"Converting torch.Tensors into jax.Arrays and vice-versa:","text":"<pre><code>import jax\nimport torch\nfrom torch_jax_interop import torch_to_jax, jax_to_torch\ntensors = {\n   \"x\": torch.randn(5),\n   \"y\": torch.arange(5),\n}\n\njax_arrays = jax.tree.map(torch_to_jax, tensors)\nprint(jax_arrays)\n# {'x': Array([-0.11146712,  0.12036294, -0.3696345 , -0.24041797, -1.1969243 ], dtype=float32),\n#  'y': Array([0, 1, 2, 3, 4], dtype=int32)}\n\ntorch_tensors = jax.tree.map(jax_to_torch, jax_arrays)\nprint(torch_tensors)\n# {'x': tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]),\n#  'y': tensor([0, 1, 2, 3, 4], dtype=torch.int32)}\n</code></pre>"},{"location":"#torch_jax_interop--using-a-jax-function-from-pytorch","title":"Using a Jax function from PyTorch:","text":"<pre><code>@jax_to_torch\ndef some_wrapped_jax_function(x: jax.Array) -&gt; jax.Array:\n    return x + jax.numpy.ones_like(x)\n\ntorch_input = torch.arange(5)\ntorch_output = some_wrapped_jax_function(torch_input)\nprint(torch_output)\n# tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n</code></pre>"},{"location":"#torch_jax_interop--using-a-torch-function-from-jax","title":"Using a Torch function from Jax:","text":"<pre><code>@torch_to_jax\ndef some_wrapped_torch_function(x: torch.Tensor) -&gt; torch.Tensor:\n    return x + torch.ones_like(x)\n\njax_input = jax.numpy.arange(5)\njax_output = some_wrapped_torch_function(jax_input)\nprint(jax_output)\n# Array([1, 2, 3, 4, 5], dtype=int32)\n</code></pre>"},{"location":"#torch_jax_interop--differentiating-through-a-jax-function-in-pytorch","title":"Differentiating through a Jax function in PyTorch:","text":"<pre><code>def some_jax_function(params: jax.Array, x: jax.Array):\n    '''Some toy function that takes in some parameters and an input vector.'''\n    return jax.numpy.dot(x, params)\n</code></pre> <p>By importing this:</p> <pre><code>from torch_jax_interop import WrappedJaxFunction\n</code></pre> <p>We can then wrap this jax function into a torch.nn.Module with learnable parameters:</p> <pre><code>module = WrappedJaxFunction(some_jax_function, jax_params=jax.random.normal(jax.random.key(0), (2, 1)))\nmodule = module.to(\"cpu\")  # jax arrays are on GPU by default, moving them to CPU for this example.\n</code></pre> <p>The parameters are now learnable parameters of the module parameters:</p> <pre><code>print(dict(module.state_dict()))\n# {'params.0': tensor([[-0.7848],\n#         [ 0.8564]])}\n</code></pre> <p>You can use this just like any other torch.nn.Module:</p> <pre><code>x, y = torch.randn(2), torch.rand(1)\noutput = module(x)\nloss = torch.nn.functional.mse_loss(output, y)\nloss.backward()\n</code></pre> <p>This also works the same way for <code>flax.linen.Module</code>s:</p> <pre><code>import flax\nclass JaxModule(flax.linen.Module):\n    output_dims: int\n    @flax.linen.compact\n    def __call__(self, x: jax.Array):\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = flax.linen.Dense(features=256)(x)\n        x = flax.linen.relu(x)\n        x = flax.linen.Dense(features=self.output_dims)(x)\n        return x\n\n\nx = jax.random.uniform(key=jax.random.key(0), shape=(16, 28, 28, 1))\njax_module = JaxModule(output_dims=10)\njax_params = jax_module.init(jax.random.key(0), x)\n</code></pre> <p>You can still of course jit your Jax code:</p> <pre><code>wrapped_jax_module = WrappedJaxFunction(jax.jit(jax_module.apply), jax_params=jax_params)\n</code></pre> <p>And you can then use this jax module in PyTorch:</p> <pre><code>x = jax_to_torch(x)\ny = torch.randint(0, 10, (16,), device=x.device)\nlogits = wrapped_jax_module(x)\nloss = torch.nn.functional.cross_entropy(logits, y, reduction=\"mean\")\nloss.backward()\nprint({name: p.grad.shape for name, p in wrapped_jax_module.named_parameters()})\n# {'params.0': torch.Size([256]), 'params.1': torch.Size([784, 256]), 'params.2': torch.Size([10]), 'params.3': torch.Size([256, 10])}\n</code></pre>"},{"location":"reference/torch_jax_interop/","title":"Torch jax interop","text":"<p>Tools to help interoperability between PyTorch and Jax code.</p>"},{"location":"reference/torch_jax_interop/#torch_jax_interop--examples","title":"Examples","text":""},{"location":"reference/torch_jax_interop/#torch_jax_interop--converting-torchtensors-into-jaxarrays-and-vice-versa","title":"Converting torch.Tensors into jax.Arrays and vice-versa:","text":"<pre><code>import jax\nimport torch\nfrom torch_jax_interop import torch_to_jax, jax_to_torch\ntensors = {\n   \"x\": torch.randn(5),\n   \"y\": torch.arange(5),\n}\n\njax_arrays = jax.tree.map(torch_to_jax, tensors)\nprint(jax_arrays)\n# {'x': Array([-0.11146712,  0.12036294, -0.3696345 , -0.24041797, -1.1969243 ], dtype=float32),\n#  'y': Array([0, 1, 2, 3, 4], dtype=int32)}\n\ntorch_tensors = jax.tree.map(jax_to_torch, jax_arrays)\nprint(torch_tensors)\n# {'x': tensor([-0.1115,  0.1204, -0.3696, -0.2404, -1.1969]),\n#  'y': tensor([0, 1, 2, 3, 4], dtype=torch.int32)}\n</code></pre>"},{"location":"reference/torch_jax_interop/#torch_jax_interop--using-a-jax-function-from-pytorch","title":"Using a Jax function from PyTorch:","text":"<pre><code>@jax_to_torch\ndef some_wrapped_jax_function(x: jax.Array) -&gt; jax.Array:\n    return x + jax.numpy.ones_like(x)\n\ntorch_input = torch.arange(5)\ntorch_output = some_wrapped_jax_function(torch_input)\nprint(torch_output)\n# tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n</code></pre>"},{"location":"reference/torch_jax_interop/#torch_jax_interop--using-a-torch-function-from-jax","title":"Using a Torch function from Jax:","text":"<pre><code>@torch_to_jax\ndef some_wrapped_torch_function(x: torch.Tensor) -&gt; torch.Tensor:\n    return x + torch.ones_like(x)\n\njax_input = jax.numpy.arange(5)\njax_output = some_wrapped_torch_function(jax_input)\nprint(jax_output)\n# Array([1, 2, 3, 4, 5], dtype=int32)\n</code></pre>"},{"location":"reference/torch_jax_interop/#torch_jax_interop--differentiating-through-a-jax-function-in-pytorch","title":"Differentiating through a Jax function in PyTorch:","text":"<pre><code>def some_jax_function(params: jax.Array, x: jax.Array):\n    '''Some toy function that takes in some parameters and an input vector.'''\n    return jax.numpy.dot(x, params)\n</code></pre> <p>By importing this:</p> <pre><code>from torch_jax_interop import WrappedJaxFunction\n</code></pre> <p>We can then wrap this jax function into a torch.nn.Module with learnable parameters:</p> <pre><code>module = WrappedJaxFunction(some_jax_function, jax_params=jax.random.normal(jax.random.key(0), (2, 1)))\nmodule = module.to(\"cpu\")  # jax arrays are on GPU by default, moving them to CPU for this example.\n</code></pre> <p>The parameters are now learnable parameters of the module parameters:</p> <pre><code>print(dict(module.state_dict()))\n# {'params.0': tensor([[-0.7848],\n#         [ 0.8564]])}\n</code></pre> <p>You can use this just like any other torch.nn.Module:</p> <pre><code>x, y = torch.randn(2), torch.rand(1)\noutput = module(x)\nloss = torch.nn.functional.mse_loss(output, y)\nloss.backward()\n</code></pre> <p>This also works the same way for <code>flax.linen.Module</code>s:</p> <pre><code>import flax\nclass JaxModule(flax.linen.Module):\n    output_dims: int\n    @flax.linen.compact\n    def __call__(self, x: jax.Array):\n        x = x.reshape((x.shape[0], -1))  # flatten\n        x = flax.linen.Dense(features=256)(x)\n        x = flax.linen.relu(x)\n        x = flax.linen.Dense(features=self.output_dims)(x)\n        return x\n\n\nx = jax.random.uniform(key=jax.random.key(0), shape=(16, 28, 28, 1))\njax_module = JaxModule(output_dims=10)\njax_params = jax_module.init(jax.random.key(0), x)\n</code></pre> <p>You can still of course jit your Jax code:</p> <pre><code>wrapped_jax_module = WrappedJaxFunction(jax.jit(jax_module.apply), jax_params=jax_params)\n</code></pre> <p>And you can then use this jax module in PyTorch:</p> <pre><code>x = jax_to_torch(x)\ny = torch.randint(0, 10, (16,), device=x.device)\nlogits = wrapped_jax_module(x)\nloss = torch.nn.functional.cross_entropy(logits, y, reduction=\"mean\")\nloss.backward()\nprint({name: p.grad.shape for name, p in wrapped_jax_module.named_parameters()})\n# {'params.0': torch.Size([256]), 'params.1': torch.Size([784, 256]), 'params.2': torch.Size([10]), 'params.3': torch.Size([256, 10])}\n</code></pre>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.torch_to_jax","title":"torch_to_jax  <code>module-attribute</code>","text":"<pre><code>torch_to_jax: Any = singledispatch(torch_to_jax)\n</code></pre> <p>Converts PyTorch tensors to JAX arrays.</p> <p>Converts the tensors \"in-place\", without the need for copies or moving data to the CPU.</p> <p>Args:   value: a torch tensor</p> <p>Returns:   a JAX array</p>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.jax_to_torch","title":"jax_to_torch  <code>module-attribute</code>","text":"<pre><code>jax_to_torch: Any = singledispatch(jax_to_torch)\n</code></pre> <p>Converts JAX arrays to PyTorch Tensors.</p> <p>Converts the tensors \"in-place\", without the need for copies or moving data to the CPU.</p> <p>Args:   value: jax array</p> <p>Returns:   a PyTorch tensor</p>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.WrappedJaxFunction","title":"WrappedJaxFunction","text":"<p>               Bases: <code>Module</code></p> <p>Wraps a jax function that returns vectors or matrices into a <code>torch.nn.Module</code>.</p> <p>This function should accept parameters as a first argument, followed by some inputs (jax.Arrays) and should return a single output (jax.Array).</p> <p>TODOs:</p> <ul> <li>[ ] Test and add support for different combinations of .requires_grad in inputs.</li> <li>[ ] Add support for multiple outputs instead of a single tensor.</li> <li>[ ] Somehow support pytrees as inputs instead of just jax Arrays, maybe with a       classmethod that flattens / unflattens stuff?</li> </ul>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.WrappedJaxFunction--examples","title":"Examples","text":"<p>Suppose we have some jax function we'd like to use in a PyTorch model:</p> <pre><code>import jax\nimport jax.numpy as jnp\ndef some_jax_function(params: jax.Array, x: jax.Array):\n    '''Some toy function that takes in some parameters and an input vector.'''\n    return jnp.dot(x, params)\n</code></pre> <p>By importing this:</p> <pre><code>from torch_jax_interop import WrappedJaxFunction\n</code></pre> <p>We can then wrap this jax function into a torch.nn.Module with learnable parameters:</p> <pre><code>import torch\nimport torch.nn\nmodule = WrappedJaxFunction(some_jax_function, jax.random.normal(jax.random.key(0), (2, 1)))\nmodule = module.to(\"cpu\")  # jax arrays are on GPU by default, moving them to CPU for this example.\n</code></pre> <p>The parameters are now learnable parameters of the module parameters:</p> <pre><code>dict(module.state_dict())\n{'params.0': tensor([[-0.7848],\n        [ 0.8564]])}\n</code></pre> <p>You can use this just like any other torch.nn.Module:</p> <pre><code>x, y = torch.randn(2), torch.rand(1)\noutput = module(x)\nloss = torch.nn.functional.mse_loss(output, y)\nloss.backward()\n\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(123, 2),\n    module,\n)\n</code></pre>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.WrappedJaxFunction.__init__","title":"__init__","text":"<pre><code>__init__(\n    jax_function: Callable[\n        [Params, *tuple[Array, ...]], Array\n    ],\n    jax_params: Params,\n    has_aux: Literal[False] = False,\n    clone_params: bool = False,\n)\n</code></pre><pre><code>__init__(\n    jax_function: Callable[\n        [Params, *tuple[Array, ...]],\n        tuple[Array, JaxPyTree],\n    ],\n    jax_params: Params,\n    has_aux: Literal[True] = True,\n    clone_params: bool = False,\n)\n</code></pre><pre><code>__init__(\n    jax_function: (\n        Callable[[Params, *tuple[Array, ...]], Array]\n        | Callable[\n            [Params, *tuple[Array, ...]],\n            tuple[Array, JaxPyTree],\n        ]\n    ),\n    jax_params: Params,\n    has_aux: bool = ...,\n    clone_params: bool = False,\n)\n</code></pre> <pre><code>__init__(\n    jax_function: (\n        Callable[[Params, *tuple[Array, ...]], Array]\n        | Callable[\n            [Params, *tuple[Array, ...]],\n            tuple[Array, JaxPyTree],\n        ]\n    ),\n    jax_params: Params,\n    has_aux: bool = False,\n    clone_params: bool = False,\n)\n</code></pre> <p>Wraps the given jax function into a torch.nn.Module.</p> <p>Parameters:</p> Name Type Description Default <code>jax_function</code> <code>Callable[[Params, *tuple[Array, ...]], Array] | Callable[[Params, *tuple[Array, ...]], tuple[Array, JaxPyTree]]</code> required <code>jax_params</code> <code>Params</code> required <code>has_aux</code> <code>bool</code> <code>False</code> <code>clone_params</code> <code>bool</code> <code>False</code>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.WrappedJaxScalarFunction","title":"WrappedJaxScalarFunction","text":"<p>               Bases: <code>WrappedJaxFunction</code></p> <p>Wraps a jax function that returns scalars into a <code>torch.nn.Module</code>.</p> <p>Compared to <code>WrappedJaxFunction</code>, this has the advantage of using jax.value_and_grad for the combined forward and backward pass.</p> <p>This function should accept parameters as a first argument, followed by some inputs (jax.Arrays) and should return a tuple with an output and some additional data (aux)</p>"},{"location":"reference/torch_jax_interop/#torch_jax_interop.torch_module_to_jax","title":"torch_module_to_jax","text":"<pre><code>torch_module_to_jax(\n    model: Module[..., Tensor],\n    example_output: Tensor | None = None,\n) -&gt; tuple[custom_vjp[Array], tuple[Array, ...]]\n</code></pre> <p>Wrap a pytorch model to be used in a jax computation.</p> <p>Copied and adapted from https://github.com/subho406/pytorch2jax/blob/main/pytorch2jax/pytorch2jax.py#L32</p> Example <pre><code>import torch\nimport jax\ntorch_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(0) # doctest:+ELLIPSIS\n# &lt;torch._C.Generator object at ...&gt;\nmodel = torch.nn.Linear(3, 2, device=torch_device)\nwrapped_model, params = torch_module_to_jax(model)\ndef loss_function(params, x: jax.Array, y: jax.Array) -&gt; jax.Array:\n    y_pred = wrapped_model(params, x)\n    return jax.numpy.mean((y - y_pred) ** 2)\nx = jax.random.uniform(key=jax.random.key(0), shape=(1, 3))\ny = jax.random.uniform(key=jax.random.key(1), shape=(1, 1))\nloss, grad = jax.value_and_grad(loss_function)(params, x, y)\nloss  # doctest: +SKIP\n# Array(0.5914371, dtype=float32)\ngrad  # doctest: +SKIP\n# (Array([[-0.02565618, -0.00836356, -0.01682458],\n#        [ 1.0495702 ,  0.34214562,  0.68827784]], dtype=float32), Array([-0.02657786,  1.0872754 ], dtype=float32))\n</code></pre> <p>To use <code>jax.jit</code> on the model, you need to pass an example of an output so we can tell the JIT compiler the output shapes and dtypes to expect:</p> <pre><code># here we reuse the same model as before:\nwrapped_model, params = torch_module_to_jax(model, example_output=torch.zeros(1, 2, device=torch_device))\ndef loss_function(params, x: jax.Array, y: jax.Array) -&gt; jax.Array:\n    y_pred = wrapped_model(params, x)\n    return jax.numpy.mean((y - y_pred) ** 2)\nloss, grad = jax.jit(jax.value_and_grad(loss_function))(params, x, y)\nloss  # doctest: +SKIP\n# Array(0.5914371, dtype=float32)\ngrad  # doctest: +SKIP\n# (Array([[-0.02565618, -0.00836356, -0.01682458],\n#        [ 1.0495702 ,  0.34214562,  0.68827784]], dtype=float32), Array([-0.02657786,  1.0872754 ], dtype=float32))\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>A Torch module.</p> required <code>example_output</code> <code>Tensor | None</code> <p>Example of an output of the model, used to specify the expected shapes and dtypes so that this computation can be jitted.</p> <code>None</code> <p>Returns:</p> Type Description <code>the functional model and the model parameters (converted to jax arrays).</code>"}]}